{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       "[torch.FloatTensor of size 5]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create uninitialized tensor\n",
    "x = torch.Tensor(5, 3)\n",
    "# create initilalized tensor\n",
    "y = torch.ones(5)\n",
    "z = torch.zeros(5)\n",
    "\n",
    "# addition\n",
    "x = torch.add(y, z)\n",
    "torch.add(y, z, out=x)\n",
    "\n",
    "# modify in place - any function that modifies has a trailing _\n",
    "x.add_(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      "[torch.DoubleTensor of size 5]\n",
      " [ 2.  2.  2.  2.  2.]\n",
      "\n",
      " 6\n",
      " 6\n",
      " 6\n",
      "[torch.FloatTensor of size 3]\n",
      " [ 6.  6.  6.]\n"
     ]
    }
   ],
   "source": [
    "# moving to and from numpy - they still share the same memory\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "b.add_(1)\n",
    "print(b, a)\n",
    "\n",
    "c = torch.ones(3)\n",
    "d = c.numpy()\n",
    "d += 5\n",
    "print(c, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "\n",
      " 8\n",
      " 8\n",
      " 8\n",
      " 8\n",
      " 8\n",
      "[torch.FloatTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# do we have a GPU?\n",
    "print(torch.cuda.is_available())\n",
    "x = torch.ones(5)\n",
    "y = torch.zeros(5) + 7\n",
    "\n",
    "# move things onto the GPU (this will fail if we don't have a GPU I think)\n",
    "x.cuda()\n",
    "y.cuda()\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Variables\n",
    "http://pytorch.org/docs/master/_modules/torch/autograd/variable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.FloatTensor'> <class 'torch.autograd.variable.Variable'>\n"
     ]
    }
   ],
   "source": [
    "# create a tensor of length 5 (column vector)\n",
    "x = torch.randn(5)\n",
    "\n",
    "# we can wrap this as a variable which adds some features we will explore in autograd\n",
    "var = autograd.Variable(x)\n",
    "print(type(x), type(var))\n",
    "\n",
    "# get back the tensor (in variable.data)\n",
    "assert id(var.data) == id(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1  1\n",
      " 1  1\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "<class 'torch.autograd.variable.Variable'> <class 'torch.autograd.variable.Variable'>\n",
      "Variable containing:\n",
      " 3  3\n",
      " 3  3\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "Variable containing:\n",
      " 27  27\n",
      " 27  27\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "Variable containing:\n",
      " 27\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 4.5000  4.5000\n",
      " 4.5000  4.5000\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Variable \"Wraps a tensor and records the operations applied to it.\"\n",
    "# http://pytorch.org/docs/0.3.0/autograd.html#torch.autograd.Variable\n",
    "x = autograd.Variable(torch.ones(2, 2), requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "y = x + 2\n",
    "print(type(y), type(x))\n",
    "print(y)\n",
    "\n",
    "z = y * y * 3\n",
    "print(z)\n",
    "\n",
    "out = z.mean()\n",
    "print(out)\n",
    "\n",
    "# Variable.backward \"Computes the gradient of current variable w.r.t. graph leaves.\"\n",
    "# http://pytorch.org/docs/0.3.0/autograd.html#torch.autograd.Variable.backward\n",
    "out.backward()\n",
    "print(x.grad) # d(out)/dx. Note that you can't do this for y or z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "Variable containing:\n",
      " -155.0617\n",
      "-1419.7878\n",
      " -276.7879\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "Variable containing:\n",
      "  51.2000\n",
      " 512.0000\n",
      "   0.0512\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3)\n",
    "x = autograd.Variable(x, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "n = 1\n",
    "while y.data.norm() < 1000:\n",
    "    n += 1\n",
    "    y = y * 2\n",
    "\n",
    "print(n)\n",
    "print(y)\n",
    "# y = 2^n * x\n",
    "# dy/dx = 2^n\n",
    "gradients = torch.FloatTensor([0.1, 1.0, 0.0001])\n",
    "y.backward(gradients)\n",
    "# x.grad = 2^n scaled by the tensor we passed to y.backward. Unclear why we do this\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "Convolutions, pooling, non-linear activation layers (relu?), linear, loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN layer weights: Parameter containing:\n",
      " 0.5076 -0.4729 -0.5471\n",
      "-0.5363  0.2122  0.4230\n",
      " 0.1265 -0.3034 -0.4931\n",
      " 0.3578 -0.0830  0.1755\n",
      "-0.1496  0.0076 -0.2130\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "NN layer biases: Parameter containing:\n",
      "-0.3478\n",
      " 0.0717\n",
      "-0.3775\n",
      "-0.0733\n",
      "-0.5364\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "Input: Variable containing:\n",
      "-0.8302  1.2876  0.5830\n",
      "-0.6887 -0.1046 -0.0343\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "Output: Variable containing:\n",
      "-1.6970  1.0368 -1.1607 -0.3749 -0.5266\n",
      "-0.6291  0.4043 -0.4160 -0.3171 -0.4268\n",
      "[torch.FloatTensor of size 2x5]\n",
      "\n",
      "[-1.69702673] -1.6970267295837402\n"
     ]
    }
   ],
   "source": [
    "# Linear Layer\n",
    "\n",
    "infeatures, outfeatures = 3, 5\n",
    "m = nn.Linear(infeatures, outfeatures)\n",
    "print(\"NN layer weights:\", m.weight)\n",
    "print(\"NN layer biases:\", m.bias)\n",
    "\n",
    "inp = autograd.Variable(torch.randn(2, 3))\n",
    "print(\"Input:\", inp)\n",
    "\n",
    "out = m(inp)\n",
    "print(\"Output:\", out)\n",
    "\n",
    "# out = m.weight . inp + m.bias; note that weight is a 5x3 and bias is a 5x1 so that \n",
    "# (weight) 5x3 . 3x1 = 5x1 + (bias) 5x1 = 5x1 output\n",
    "\n",
    "# Sanity check that we know what is going on\n",
    "# First row of weights is dotted again column input vector to get first element of column output vector.\n",
    "# Add weight to this.\n",
    "print(np.dot(m.weight.data[0].numpy(), inp.data[0].numpy()) + m.bias[0].data.numpy(), out.data[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.117742985487 -0.11774298548698425\n",
      "0.37167018652 0.3716701865196228\n"
     ]
    }
   ],
   "source": [
    "# Convolution layer\n",
    "\n",
    "# \"channel\" is ~ attribute of data\n",
    "# 1 channel - grayscale image. 2 channels - height and weight of person. 3 channels - RGB image\n",
    "# 1d here means that the vector is 1d (just has a length)\n",
    "# This mostly just changes the number of channels, but also changes the size of the data based off how many\n",
    "# full convolutions it can fit\n",
    "\n",
    "in_channels, out_channels, kernel_size = 2, 3, 4\n",
    "m = nn.Conv1d(in_channels, out_channels, kernel_size)\n",
    "assert m.weight.shape == torch.Size([out_channels, in_channels, kernel_size])\n",
    "assert m.bias.shape == torch.Size([out_channels])\n",
    "\n",
    "# 4 variables that have 2 channel of 8 items\n",
    "var_num, var_channels, var_length = 4, in_channels, 8\n",
    "inp = autograd.Variable(torch.randn(var_num, var_channels, var_length))\n",
    "\n",
    "out = m(inp)\n",
    "assert out.shape == torch.Size([var_num, out_channels, var_length - (kernel_size - 1)])\n",
    "\n",
    "# So for the first piece of data (2 channels, 8 items)\n",
    "# We get out (3 channels, 5 items)\n",
    "# This is the convolution that gets us the first data point in the first output channel\n",
    "print(\n",
    "    np.dot(inp.data[0][0][:4], m.weight.data[0][0]) + # convolution over 0th inchannel to 0th outchannel\n",
    "    np.dot(inp.data[0][1][:4], m.weight.data[0][1]) + # convolution over 1st inchannel to 0th outchannel\n",
    "    m.bias.data[0], # + 0th bias\n",
    "    out.data[0][0][0])\n",
    "# Similarly, to get the last data point in the 3rd output channel (still in first input variable)\n",
    "print(\n",
    "    np.dot(inp.data[0][0][-4:], m.weight.data[2][0]) + \n",
    "    np.dot(inp.data[0][1][-4:], m.weight.data[2][1]) +\n",
    "    m.bias.data[2],\n",
    "    out.data[0][2][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transposed convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09894981980323792 Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.8950\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "\n",
      " 0.1604\n",
      "-0.2147\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "-0.4076274335384369 -0.407627433538\n"
     ]
    }
   ],
   "source": [
    "in_channels, out_channels, kernel_size = 2, 3, 4\n",
    "m = nn.ConvTranspose1d(in_channels, out_channels, kernel_size)\n",
    "\n",
    "assert m.weight.shape == torch.Size([in_channels, out_channels, kernel_size]) # in and out are switch vs above\n",
    "assert m.bias.shape == torch.Size([out_channels]) # same as above\n",
    "\n",
    "# 4 variables that have 2 channel of 8 items\n",
    "var_num, var_channels, var_length = 4, in_channels, 8\n",
    "inp = autograd.Variable(torch.randn(var_num, var_channels, var_length))\n",
    "assert inp.shape == torch.Size([var_num, in_channels, var_length])\n",
    "\n",
    "out = m(inp)\n",
    "# - has become a plus. Input gets longer!\n",
    "assert out.shape == torch.Size([var_num, out_channels, var_length + (kernel_size - 1)])\n",
    "\n",
    "out0, inp0 = out.data[0], inp.data[0]\n",
    "print(out0[0][0], \n",
    "      (inp0[0][0] * m.weight[0][0][0]) + # So it appears that the weights are indexes backwards?\n",
    "      (inp0[1][0] * m.weight[1][0][0]) + \n",
    "      m.bias[0])\n",
    "\n",
    "print(m.weight.data[0][0][0:2])\n",
    "\n",
    "print(out0[0][1], \n",
    "      np.dot(inp0[0][0:2], m.weight.data[0][0][0:2].numpy()[::-1]) + # Hack to reverse...\n",
    "      np.dot(inp0[1][0:2], m.weight.data[1][0][0:2].numpy()[::-1]) + \n",
    "      m.bias.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Pool layer\n",
    "\n",
    "kernel_size = 2\n",
    "m = nn.MaxPool1d(2) # Can also average pool etc\n",
    "\n",
    "var_num, var_channels, var_length = 4, 2, 8\n",
    "inp = autograd.Variable(torch.randn(var_num, var_channels, var_length))\n",
    "\n",
    "out = m(inp)\n",
    "\n",
    "assert out.data[0][0][0] == np.max(inp.data[0][0][0:kernel_size].numpy())\n",
    "assert out.data[3][1][3] == np.max(inp.data[3][1][3*kernel_size:3*kernel_size+kernel_size].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Non linear layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.1145 -1.8109  1.0012\n",
      "-0.0845 -0.0879  0.0550\n",
      "[torch.FloatTensor of size 2x3]\n",
      " Variable containing:\n",
      " 0.1145  0.0000  1.0012\n",
      " 0.0000  0.0000  0.0550\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Non linear layers\n",
    "\n",
    "m = nn.ReLU() # can also sigmoid, softmax etc\n",
    "inp = autograd.Variable(torch.randn(2, 3))\n",
    "out = m(inp)\n",
    "print(inp, out) # just coverts elementwise to max(0, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Functional\n",
    "\n",
    "In the NN layer we have been defining full layers. These are things that are trained (have weights that will be updated). We also just want to do purely arithmetic things (actually the ReLU was like that). For these purely arithmetic things there is `torch.nn.Functional`. See discussion https://discuss.pytorch.org/t/how-to-choose-between-torch-nn-functional-and-torch-nn-module/2800 for a bit more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1.3510 -0.7928  0.7239\n",
      " 0.3445  0.0012 -0.4069\n",
      "[torch.FloatTensor of size 2x3]\n",
      " Variable containing:\n",
      " 1.3510  0.0000  0.7239\n",
      " 0.3445  0.0012  0.0000\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Functional layers\n",
    "\n",
    "inp = autograd.Variable(torch.randn(2, 3))\n",
    "out = F.relu(inp)\n",
    "print(inp, out) # just coverts elementwise to max(0, x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
