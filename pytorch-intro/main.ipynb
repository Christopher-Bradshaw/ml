{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       "[torch.FloatTensor of size 5]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create uninitialized tensor\n",
    "x = torch.Tensor(5, 3)\n",
    "# create initilalized tensor\n",
    "y = torch.ones(5)\n",
    "z = torch.zeros(5)\n",
    "\n",
    "# addition\n",
    "x = torch.add(y, z)\n",
    "torch.add(y, z, out=x)\n",
    "\n",
    "# modify in place - any function that modifies has a trailing _\n",
    "x.add_(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      "[torch.DoubleTensor of size 5]\n",
      " [ 2.  2.  2.  2.  2.]\n",
      "\n",
      " 6\n",
      " 6\n",
      " 6\n",
      "[torch.FloatTensor of size 3]\n",
      " [ 6.  6.  6.]\n"
     ]
    }
   ],
   "source": [
    "# moving to and from numpy - they still share the same memory\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "b.add_(1)\n",
    "print(b, a)\n",
    "\n",
    "c = torch.ones(3)\n",
    "d = c.numpy()\n",
    "d += 5\n",
    "print(c, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "\n",
      " 8\n",
      " 8\n",
      " 8\n",
      " 8\n",
      " 8\n",
      "[torch.FloatTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# do we have a GPU?\n",
    "print(torch.cuda.is_available())\n",
    "x = torch.ones(5)\n",
    "y = torch.zeros(5) + 7\n",
    "\n",
    "# move things onto the GPU (this will fail if we don't have a GPU I think)\n",
    "x.cuda()\n",
    "y.cuda()\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1  1\n",
      " 1  1\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "Variable containing:\n",
      " 3  3\n",
      " 3  3\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "Variable containing:\n",
      " 27  27\n",
      " 27  27\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "Variable containing:\n",
      " 27\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 4.5000  4.5000\n",
      " 4.5000  4.5000\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Variable \"Wraps a tensor and records the operations applied to it.\"\n",
    "# http://pytorch.org/docs/0.3.0/autograd.html#torch.autograd.Variable\n",
    "x = Variable(torch.ones(2, 2), requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "y = x + 2\n",
    "print(y)\n",
    "\n",
    "z = y * y * 3\n",
    "print(z)\n",
    "\n",
    "out = z.mean()\n",
    "print(out)\n",
    "\n",
    "# Variable.backward \"Computes the gradient of current variable w.r.t. graph leaves.\"\n",
    "# http://pytorch.org/docs/0.3.0/autograd.html#torch.autograd.Variable.backward\n",
    "out.backward()\n",
    "print(x.grad) # d(out)/dx. Note that you can't do this for y or z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "Variable containing:\n",
      " 378.0182\n",
      " 807.5120\n",
      "-659.2539\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "Variable containing:\n",
      "  51.2000\n",
      " 512.0000\n",
      "   0.0512\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3)\n",
    "x = Variable(x, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "n = 1\n",
    "while y.data.norm() < 1000:\n",
    "    n += 1\n",
    "    y = y * 2\n",
    "\n",
    "print(n)\n",
    "print(y)\n",
    "# y = 2^n * x\n",
    "# dy/dx = 2^n\n",
    "gradients = torch.FloatTensor([0.1, 1.0, 0.0001])\n",
    "y.backward(gradients)\n",
    "# x.grad = 2^n scaled by the tensor we passed to y.backward. Unclear why we do this\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
